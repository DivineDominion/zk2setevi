@book{FastZettelkastenmethodeKontrollieredein2015,
  title = {{Die Zettelkastenmethode: Kontrolliere dein Wissen}},
  isbn = {978-1-5177-3431-2},
  shorttitle = {{Die Zettelkastenmethode}},
  abstract = {Souver{\"a}nit{\"a}t ist *die* Schl{\"u}sselkompetenz in unserer modernen Welt im Umgang mit Wissen und Informationen.   Dabei sind Schnelligkeit, Sicherheit und Flexibilit{\"a}t die entscheidenden Komponenten, um diese Souver{\"a}nit{\"a}t f{\"u}r sich zu gewinnen.  Die Zettelkastenmethode ist eine flexible und individuell anpassbare Technik mit Wissen umzugehen. Sie ist auf jeden System mit ein paar Handgriffen realisierbar, ohne dass man komplizierte Programme verstehen muss.   In ihrer Einfachheit ist sie un{\"u}bertroffen und ist der (fast) unsichtbare Helfer f{\"u}r dich im Umgang mit dem Wissen.  Niklas Luhmann, einer der produktivsten Wissenschaftler aller Zeiten, sagte einmal, er m{\"u}sse gar nicht schreiben, das mache der Zettelkasten f{\"u}r ihn.   Als ich das Prinzip hinter seinem Zettelkasten, zusammen mit Christian Tietze, in die moderne Welt der digitalen Technik {\"u}bertrug, war ich {\"u}berrascht, wie viel Wahrheit in Luhmanns Aussage steckt.},
  language = {Deutsch},
  publisher = {{CreateSpace Independent Publishing Platform}},
  author = {Fast, Sascha and Tietze, Christian},
  month = oct,
  year = {2015}
}

@book{AhrensHowTakeSmart2017,
  edition = {1},
  title = {{How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking \textendash{} for Students, Academics and Nonfiction Book Writers}},
  isbn = {978-1-5428-6650-7},
  shorttitle = {{How to Take Smart Notes}},
  abstract = {The key to good and efficient writing lies in the intelligent organisation of ideas and notes. This book helps students, academics and nonfiction writers to get more done, write intelligent texts and learn for the long run. It teaches you how to take smart notes and ensure they bring you and your projects forward. The Take Smart Notes principle is based on established psychological insight and draws from a tried and tested note-taking-technique. This is the first comprehensive guide and description of this system in English, and not only does it explain how it works, but also why. It suits students and academics in the social sciences and humanities, nonfiction writers and others who are in the business of reading, thinking and writing. Instead of wasting your time searching for notes, quotes or references, you can focus on what really counts: thinking, understanding and developing new ideas in writing. It does not matter if you prefer taking notes with pen and paper or on a computer, be it Windows, Mac or Linux. And you can start right away.},
  language = {Englisch},
  publisher = {{CreateSpace Independent Publishing Platform}},
  author = {Ahrens, S{\"o}nke},
  month = feb,
  year = {2017}
}

@article{hackel_semantic3d.net_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.03847},
  primaryClass = {cs},
  title = {{{Semantic3D}}.Net: {{A}} New {{Large}}-Scale {{Point Cloud Classification Benchmark}}},
  shorttitle = {{{Semantic3D}}.Net},
  abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
  urldate = {2017-12-10},
  url = {http://arxiv.org/abs/1704.03847},
  journal = {arXiv:1704.03847 [cs]},
  author = {Hackel, Timo and Savinov, Nikolay and Ladicky, Lubor and Wegner, Jan D. and Schindler, Konrad and Pollefeys, Marc},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/Users/rs/Zotero/storage/TRGUFQWK/Hackel et al. - 2017 - Semantic3D.net A new Large-scale Point Cloud Clas.pdf;/Users/rs/Zotero/storage/TABZ6WFL/1704.html},
  annote = {Comment: Accepted to ISPRS Annals. The benchmark website is available at http://www.semantic3d.net/ . The baseline code is available at https://github.com/nsavinov/semantic3dnet}
}

@article{LinFeaturePyramidNetworks2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.03144},
  primaryClass = {cs},
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  urldate = {2017-12-10},
  url = {http://arxiv.org/abs/1612.03144},
  journal = {arXiv:1612.03144 [cs]},
  author = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rs/Zotero/storage/SVCFCK9P/Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf;/Users/rs/Zotero/storage/IDYEUHRS/1612.html}
}

@article{caelles_one-shot_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05198},
  primaryClass = {cs},
  title = {One-{{Shot Video Object Segmentation}}},
  abstract = {This paper tackles the task of semi-supervised video object segmentation, i.e., the separation of an object from the background in a video, given the mask of the first frame. We present One-Shot Video Object Segmentation (OSVOS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one-shot). Although all frames are processed independently, the results are temporally coherent and stable. We perform experiments on two annotated video segmentation databases, which show that OSVOS is fast and improves the state of the art by a significant margin (79.8\% vs 68.0\%).},
  urldate = {2017-12-10},
  url = {http://arxiv.org/abs/1611.05198},
  journal = {arXiv:1611.05198 [cs]},
  author = {Caelles, Sergi and Maninis, Kevis-Kokitsi and Pont-Tuset, Jordi and Leal-Taix{\'e}, Laura and Cremers, Daniel and Van Gool, Luc},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rs/Zotero/storage/QW8FL79D/Caelles et al. - 2016 - One-Shot Video Object Segmentation.pdf;/Users/rs/Zotero/storage/96DSJETN/1611.html},
  annote = {Comment: CVPR 2017 camera ready. Code: http://www.vision.ee.ethz.ch/\textasciitilde{}cvlsegmentation/osvos/}
}

@article{qi_pointnet_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.00593},
  primaryClass = {cs},
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  urldate = {2017-12-10},
  url = {http://arxiv.org/abs/1612.00593},
  journal = {arXiv:1612.00593 [cs]},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rs/Zotero/storage/STAU3NTZ/Qi et al. - 2016 - PointNet Deep Learning on Point Sets for 3D Class.pdf;/Users/rs/Zotero/storage/9R6FFDB8/1612.html},
  annote = {Comment: CVPR 2017}
}

@article{huang_densely_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06993},
  primaryClass = {cs},
  title = {Densely {{Connected Convolutional Networks}}},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
  urldate = {2017-12-10},
  url = {http://arxiv.org/abs/1608.06993},
  journal = {arXiv:1608.06993 [cs]},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and {van der Maaten}, Laurens},
  month = aug,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {/Users/rs/Zotero/storage/HENB3EPE/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf;/Users/rs/Zotero/storage/E4MZFEB3/1608.html},
  annote = {Comment: 12 pages}
}

@inproceedings{VedantamCIDErConsensusBasedImage2015,
  title = {{{CIDEr}}: {{Consensus}}-{{Based Image Description Evaluation}}},
  shorttitle = {{{CIDEr}}},
  urldate = {2018-01-02},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.html},
  author = {Vedantam, Ramakrishna and Lawrence Zitnick, C. and Parikh, Devi},
  year = {2015},
  pages = {4566--4575},
  file = {/Users/rs/Zotero/storage/YVNBT2YI/Vedantam et al. - 2015 - CIDEr Consensus-Based Image Description Evaluatio.pdf;/Users/rs/Zotero/storage/WB6PEY5R/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.html}
}

@inproceedings{LavieMeteorAutomaticMetric2007,
  address = {Stroudsburg, PA, USA},
  series = {StatMT '07},
  title = {Meteor: {{An Automatic Metric}} for {{MT Evaluation}} with {{High Levels}} of {{Correlation}} with {{Human Judgments}}},
  shorttitle = {Meteor},
  abstract = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.},
  urldate = {2018-01-02},
  url = {http://dl.acm.org/citation.cfm?id=1626355.1626389},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Statistical Machine Translation}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Lavie, Alon and Agarwal, Abhaya},
  year = {2007},
  pages = {228--231},
  file = {/Users/rs/Zotero/storage/MAZFC6LR/Lavie und Agarwal - 2007 - Meteor An Automatic Metric for MT Evaluation with.pdf}
}

@article{KilickayaReevaluatingAutomaticMetrics2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.07600},
  primaryClass = {cs},
  title = {Re-Evaluating {{Automatic Metrics}} for {{Image Captioning}}},
  abstract = {The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover's Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.},
  urldate = {2018-01-02},
  url = {http://arxiv.org/abs/1612.07600},
  journal = {arXiv:1612.07600 [cs]},
  author = {Kilickaya, Mert and Erdem, Aykut and Ikizler-Cinbis, Nazli and Erdem, Erkut},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computation and Language},
  file = {/Users/rs/Zotero/storage/UICYTL93/Kilickaya et al. - 2016 - Re-evaluating Automatic Metrics for Image Captioni.pdf;/Users/rs/Zotero/storage/ZKGFU88L/1612.html}
}

@inproceedings{PapineniBLEUMethodAutomatic2002,
  title = {{{BLEU}}: A {{Method}} for {{Automatic Evaluation}} of {{Machine Translation}}},
  shorttitle = {{{BLEU}}},
  abstract = {Human evaluations of machine translation  are extensive but expensive. Human evaluations  can take months to finish and involve  human labor that can not be reused.},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-jing},
  year = {2002},
  pages = {311--318},
  file = {/Users/rs/Zotero/storage/Q8JF7FI3/Papineni et al. - 2002 - BLEU a Method for Automatic Evaluation of Machine.pdf;/Users/rs/Zotero/storage/TKI7Z97L/summary.html}
}

@inproceedings{VinyalsShowTellNeural2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  urldate = {2018-01-02},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2015},
  pages = {3156--3164},
  file = {/Users/rs/Zotero/storage/MPA4MFR9/Vinyals et al. - 2015 - Show and Tell A Neural Image Caption Generator.pdf;/Users/rs/Zotero/storage/4JECSF8S/Vinyals_Show_and_Tell_2015_CVPR_paper.html}
}

@inproceedings{ChenMindEyeRecurrent2015,
  title = {Mind's {{Eye}}: {{A Recurrent Visual Representation}} for {{Image Caption Generation}}},
  shorttitle = {Mind's {{Eye}}},
  urldate = {2018-01-02},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Chen_Minds_Eye_A_2015_CVPR_paper.html},
  author = {Chen, Xinlei and Lawrence Zitnick, C.},
  year = {2015},
  pages = {2422--2431},
  file = {/Users/rs/Zotero/storage/RE5HBZ5R/Chen und Lawrence Zitnick - 2015 - Mind's Eye A Recurrent Visual Representation for .pdf;/Users/rs/Zotero/storage/M77ZZDNJ/Chen_Minds_Eye_A_2015_CVPR_paper.html}
}

@article{TeneyZeroShotVisualQuestion2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05546},
  primaryClass = {cs},
  title = {Zero-{{Shot Visual Question Answering}}},
  abstract = {Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-the-art performance in the standard VQA evaluation setting.},
  urldate = {2018-01-11},
  url = {http://arxiv.org/abs/1611.05546},
  journal = {arXiv:1611.05546 [cs]},
  author = {Teney, Damien and van den Hengel, Anton},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  file = {/Users/rs/Zotero/storage/YHWVEAVM/Teney und Hengel - 2016 - Zero-Shot Visual Question Answering.pdf;/Users/rs/Zotero/storage/YIR4ZBNR/1611.html}
}

@article{TeneyTipsTricksVisual2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.02711},
  primaryClass = {cs},
  title = {Tips and {{Tricks}} for {{Visual Question Answering}}: {{Learnings}} from the 2017 {{Challenge}}},
  shorttitle = {Tips and {{Tricks}} for {{Visual Question Answering}}},
  abstract = {This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.},
  urldate = {2018-01-11},
  url = {http://arxiv.org/abs/1708.02711},
  journal = {arXiv:1708.02711 [cs]},
  author = {Teney, Damien and Anderson, Peter and He, Xiaodong and van den Hengel, Anton},
  month = aug,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computation and Language},
  file = {/Users/rs/Zotero/storage/CHTRD4MX/Teney et al. - 2017 - Tips and Tricks for Visual Question Answering Lea.pdf;/Users/rs/Zotero/storage/LYNISBMB/1708.html},
  annote = {Comment: Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR}
}

@article{AkibaExtremelyLargeMinibatch2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.04325},
  primaryClass = {cs},
  title = {Extremely {{Large Minibatch SGD}}: {{Training ResNet}}-50 on {{ImageNet}} in 15 {{Minutes}}},
  shorttitle = {Extremely {{Large Minibatch SGD}}},
  abstract = {We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be achieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by using a large minibatch size of 32k. To maintain accuracy with this large minibatch size, we employed several techniques such as RMSprop warm-up, batch normalization without moving averages, and a slow-start learning rate schedule. This paper also describes the details of the hardware and software of the system used to achieve the above performance.},
  urldate = {2018-01-16},
  url = {http://arxiv.org/abs/1711.04325},
  journal = {arXiv:1711.04325 [cs]},
  author = {Akiba, Takuya and Suzuki, Shuji and Fukuda, Keisuke},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/rs/Zotero/storage/D6WRVHFY/Akiba et al. - 2017 - Extremely Large Minibatch SGD Training ResNet-50 .pdf;/Users/rs/Zotero/storage/RP5LG25F/1711.html},
  annote = {Comment: NIPS'17 Workshop: Deep Learning at Supercomputer Scale}
}

@article{GoyalAccurateLargeMinibatch2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02677},
  primaryClass = {cs},
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde{}90\% scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.},
  urldate = {2018-01-16},
  url = {http://arxiv.org/abs/1706.02677},
  journal = {arXiv:1706.02677 [cs]},
  author = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/rs/Zotero/storage/HVFG397A/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;/Users/rs/Zotero/storage/U6T2AVPX/1706.html},
  annote = {Comment: Tech report}
}

@misc{JainFastTextGensimword,
  title = {{{FastText}} and {{Gensim}} Word Embeddings | {{RaRe Technologies}}},
  urldate = {2018-02-06},
  url = {https://rare-technologies.com/fasttext-and-gensim-word-embeddings/},
  author = {Jain, Jayant},
  file = {/Users/rs/Zotero/storage/I46BD8I3/fasttext-and-gensim-word-embeddings.html}
}

@article{XuReviseSaturatedActivation2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.05980},
  primaryClass = {cs},
  title = {Revise {{Saturated Activation Functions}}},
  abstract = {In this paper, we revise two commonly used saturated functions, the logistic sigmoid and the hyperbolic tangent (tanh). We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible reason making training deep networks with the logistic function difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid achieves comparable results with tanh. Then following the same argument, we improve tahn by penalizing in the negative part. We show that "penalized tanh" is comparable and even outperforms the state-of-the-art non-saturated functions including ReLU and leaky ReLU on deep convolution neural networks. Our results contradict to the conclusion of previous works that the saturation property causes the slow convergence. It suggests further investigation is necessary to better understand activation functions in deep architectures.},
  urldate = {2018-02-06},
  url = {http://arxiv.org/abs/1602.05980},
  journal = {arXiv:1602.05980 [cs]},
  author = {Xu, Bing and Huang, Ruitong and Li, Mu},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Learning},
  file = {/Users/rs/Zotero/storage/U62JGXFH/Xu et al. - 2016 - Revise Saturated Activation Functions.pdf;/Users/rs/Zotero/storage/V2QL759U/1602.html}
}

@misc{HowCognitiveFluencya,
  title = {How {{Cognitive Fluency Affects Decision Making}} :: {{UXmatters}}},
  urldate = {2018-02-07},
  url = {https://www.uxmatters.com/mt/archives/2011/07/how-cognitive-fluency-affects-decision-making.php},
  file = {/Users/rs/Zotero/storage/ARBQ2IZF/how-cognitive-fluency-affects-decision-making.html}
}

@article{HarzigMultimodalImageCaptioning2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.01958},
  primaryClass = {cs},
  title = {Multimodal {{Image Captioning}} for {{Marketing Analysis}}},
  abstract = {Automatically captioning images with natural language sentences is an important research topic. State of the art models are able to produce human-like sentences. These models typically describe the depicted scene as a whole and do not target specific objects of interest or emotional relationships between these objects in the image. However, marketing companies require to describe these important attributes of a given scene. In our case, objects of interest are consumer goods, which are usually identifiable by a product logo and are associated with certain brands. From a marketing point of view, it is desirable to also evaluate the emotional context of a trademarked product, i.e., whether it appears in a positive or a negative connotation. We address the problem of finding brands in images and deriving corresponding captions by introducing a modified image captioning network. We also add a third output modality, which simultaneously produces real-valued image ratings. Our network is trained using a classification-aware loss function in order to stimulate the generation of sentences with an emphasis on words identifying the brand of a product. We evaluate our model on a dataset of images depicting interactions between humans and branded products. The introduced network improves mean class accuracy by 24.5 percent. Thanks to adding the third output modality, it also considerably improves the quality of generated captions for images depicting branded products.},
  urldate = {2018-02-08},
  url = {http://arxiv.org/abs/1802.01958},
  journal = {arXiv:1802.01958 [cs]},
  author = {Harzig, Philipp and Brehm, Stephan and Lienhart, Rainer and Kaiser, Carolin and Schallner, Ren{\'e}},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rs/Zotero/storage/64JDECKH/Harzig et al. - 2018 - Multimodal Image Captioning for Marketing Analysis.pdf;/Users/rs/Zotero/storage/STV396X2/1802.html},
  annote = {Comment: 4 pages, 1 figure, accepted at MIPR2018}
}

@article{ZornZettelkastenGesellschaftMedientheorie,
  title = {Der {{Zettelkasten}} Der {{Gesellschaft}}. {{Medientheorie}} Als {{Gesellschaftstheorie}}: {{Eine Luhmann}}-{{Relekt{\"u}re}}, {{The Card}}-{{File System}} of the {{Society}}. {{Media Theory}} as a {{Theory}} of {{Society}}: {{A Re}}-{{Reading}} of {{Luhmann}}},
  shorttitle = {Der {{Zettelkasten}} Der {{Gesellschaft}}. {{Medientheorie}} Als {{Gesellschaftstheorie}}},
  abstract = {Die Dissertation hat die \textendash{} gegenw{\"a}rtig noch buchst{\"a}blich \guilsinglright{}gest{\"u}ckelte\guilsinglleft{} \textendash{} Medientheorie Niklas Luhmanns zum Gegenstand, seine zum Teil ausgesprochen heterogenen und bislang oft untereinander noch ganz unverbundenen Medienbegriffe bzw. medientheoretischen Teilst{\"u}cke also: den funktionalen Bezug des Medienbegriffs auf die \guillemotright{}Unwahrscheinlichkeit von Kommunikation\guillemotleft; die Theorie der \guillemotright{}Verbreitungs-\guillemotleft{} und die der \guillemotright{}Erfolgsmedien\guillemotleft; die Medium/Form-Unterscheidung; die Behandlung von Medien als \guillemotright{}strukturelle Kopplungen\guillemotleft{} zwischen dem Gesellschaftssystem und seiner (vor allem menschlichen) Umwelt. Ziel der Arbeit ist es zun{\"a}chst, eine ganze Reihe bisher ungenutzt gebliebener kultur- und gesellschaftstheoretischer Erkl{\"a}rungspotentiale dieser Medientheorie(n) zu erschlie{\ss}en, also sichtbar und auch nutzbar zu machen. So wird im Laufe der Untersuchung dann aber vor allem deutlich, dass Luhmann mit seinen vielf{\"a}ltigen Anschnitten (seiner \guilsinglright{}Verzettelung\guilsinglleft{} gewisserma{\ss}en) des Medienbegriffs die Grundlagen f{\"u}r ein eigenes gesellschaftstheoretisches Programm gelegt hat: Es zeigt sich, dass sich auf ihrer Grundlage ein eigenst{\"a}ndiger Zusammenhang von gesellschaftstheoretischen Problemstellungen und Hypothesen entwickeln l{\"a}sst. Und dieses Programm erweist sich zudem insoweit auch als ein Gegenprojekt zum bisherigen gesellschaftstheoretischen Programm Luhmanns, als es unter anderem nahe legt, dass die Untersuchung der aktuellen gesellschaftlichen Differenzierungsform (\guillemotright{}funktionale Differenzierung\guillemotleft), auf die Luhmann sich noch konzentriert hatte, eine \textendash{} wenigstens langfristig \textendash{} gesellschaftstheoretisch weitaus weniger aufschlussreiche Programmatik darstellt, als eine systematisierte Untersuchung medial bedingter evolution{\"a}rer Trends im Gesellschaftssystem etwa sie darstellen w{\"u}rde. Um die in den vielf{\"a}ltigen Medientheorien Luhmanns bislang erst in \guilsinglright{}verzettelter\guilsinglleft{} Form sozusagen vorliegende alternative Theorieprogrammatik allm{\"a}hlich lesbar werden zu lassen, konzentriert sich die Arbeit zun{\"a}chst darauf, neue Zusammenh{\"a}nge und noch ungesehene Beziehungen zwischen den genannten Teilst{\"u}cken von Luhmanns Medientheorie heraus zu arbeiten \textendash{} in genauen Lekt{\"u}ren aller einschl{\"a}gigen Texte Luhmanns zu diesen \guilsinglright{}Theoriest{\"u}cken\guilsinglleft{} sowie vieler verstreuter Passagen, die sie im Kontext anderer Fragen und \guilsinglright{}Theoriest{\"u}cke\guilsinglleft{} behandeln. Um Luhmanns Medientheorie(n) als sein zweites Gesellschaftstheorie-Projekt, eine \textendash{} zum Teil konkurrierende \textendash{} Theorie in der Theorie gewisserma{\ss}en lesbar machen zu k{\"o}nnen, bedient die Arbeit sich au{\ss}erdem eines spezifisch kulturwissenschaftlichen Verfahrens der Text-Lekt{\"u}re. Ein bestimmter Ausschnitt von Luhmanns Wirken und Werk wird als Schl{\"u}ssel zum Zusammenhang dieses Alternativprojekts gelesen: Man findet diesen theorieprogrammatischen Zusammenhang, so wird gezeigt, sozusagen vorweg genommen, allegorisch verdichtet und sinnbildlich verk{\"o}rpert in Luhmanns \guilsinglright{}Zettelkasteniana\guilsinglleft{} \textendash{} in seinen legend{\"a}ren Bemerkungen zu seinem Zettelkasten (in zahlreichen Interviews, aber auch in einer eigens diesem Thema gewidmeten Publikation) also sowie vor allem im dort dargelegten Anteil dieses \guillemotright{}Universalinstruments\guillemotleft{} (Luhmann) an der Theorieentwicklung. Wertvoll werden die \guilsinglright{}Zettelkasteniana\guilsinglleft, genauer gesagt, vor allem wenn man die darin dargelegten Verh{\"a}ltnisse zwischen Zettelkasten, Theorieentwicklung und dem Theorie-Autor Luhmann als allegorische Veranschaulichung der \textendash{} in Luhmanns Theorie verhandelten \textendash{} Verh{\"a}ltnisse zwischen den Medien, dem Gesellschaftssystem und dessen (vor allem menschlicher) Umwelt liest: Nimmt man zur Kenntnis, wie sich demnach sein eigener Anteil und der Anteil seines Zettelkasten an der Theorieentwicklung allm{\"a}hlich verschoben hat (je l{\"a}nger Luhmann mit dem Zettelkasten arbeitete), so werden vor allem alle Theoreme und Formulierungen Luhmanns in noch einmal ganz anderer Weise entzifferbar, die medial bedingte evolution{\"a}re Ver{\"a}nderungen im Verh{\"a}ltnis zwischen Gesellschaftssystem und menschlicher Umwelt behandeln. Im Sinne und im Dienste der Grundhypothese \textendash{} wonach in Luhmanns Medientheorien die Grundlagen gelegt sind f{\"u}r die Ausarbeitung einer \guilsinglright{}anderen\guilsinglleft{} Gesellschaftstheorie (einer Gesellschafts- als Medien-, bzw. einer Medien- als Gesellschaftstheorie) \textendash{} richten sich die Suchbewegungen der Studie aber auch noch in andere Richtungen. Es geht dar{\"u}ber hinaus ganz allgemein darum, Kontingenzen in der Architektur wie in der Formulierung der Theorie auf eine entsprechende alternative Ausarbeitung hin zu befragen. So geht es etwa auch darum, dass Luhmanns Gesellschaftstheorie in ihrer aktuellen Anlage schon {\"u}berall, in all ihren zentralen Problemstellungen, ganz unmittelbar auf den Medienbegriff als zentrale Antwort verweist. Und nicht zuletzt geht es darum, \guillemotright{}Mediengesellschaftswissenschaft\guillemotleft{} sozusagen als eine m{\"o}gliche Alternative zur heutigen \guillemotright{}Medienkulturwissenschaft\guillemotleft{} kenntlich zu machen.},
  language = {en},
  urldate = {2018-02-17},
  url = {https://opus4.kobv.de/opus4-euv/frontdoor/index/index/docId/24},
  author = {Zorn, Carsten},
  file = {/Users/rs/Zotero/storage/APSWUQRL/DerZettelkastenDerGesellschaftDRUCKV.pdf;/Users/rs/Zotero/storage/9ENMG2XP/24.html}
}
